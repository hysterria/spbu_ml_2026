{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HERrGPm5IJBT"
   },
   "source": [
    "# Введение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SmWz4pRoYww"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes, fetch_openml,load_iris,fetch_california_housing\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression, RFE, SelectFromModel, SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import (\n",
    "RepeatedStratifiedKFold, \n",
    "cross_val_score, \n",
    "train_test_split, \n",
    "GridSearchCV,\n",
    "cross_val_predict, \n",
    "learning_curve, \n",
    "validation_curve)\n",
    "from sklearn.linear_model import LinearRegression,Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error,zero_one_loss, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from os.path import join as pjoin\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "#sharper plots\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV,\n",
    "                                  SGDClassifier)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:/data/ml\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-Tbl_ahIEfz"
   },
   "source": [
    "## Проклятие размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkemB_fZp687"
   },
   "source": [
    " Проклятие размерности означает, что по мере увеличения количества объектов или измерений в наборе данных объем данных, необходимый для эффективного моделирования взаимосвязи между объектами и целевой переменной, растет экспоненциально.\n",
    "\n",
    "Чем больше мы добавляем признаков, не увеличивая объем данных, используемых для обучения модели, тем больше увеличивается среднее расстояние между точками в пространстве признаков. Из-за такой разреженности становится гораздо проще найти удобное и совершенное, но не столь оптимальное решение для модели машинного обучения. Следовательно, модель плохо обобщает, что делает прогнозы ненадежными. Это может привести к таким проблемам, как переобучение и снижение точности.\n",
    "\n",
    "Чтобы смягчить проклятие размерности, можно использовать такие методы, как выбор признаков, уменьшение размерности и ансамблевые методы.\n",
    "\n",
    "Для примера рассмотрим 1000 случайных точек в пространствах размерности от 2 до 50. Построим график, показывающий это свойство."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "FhEx6m0spT00",
    "outputId": "92846944-c155-4504-8906-f327944ddaa7"
   },
   "outputs": [],
   "source": [
    "deltas = []\n",
    "for N in range(2,50):\n",
    "    # Generate 1000 random points in N dimensions.\n",
    "    P = [np.random.randint(-100, 100, N) for _ in range(10000)]\n",
    "    Q = np.random.randint(-100,100, N)\n",
    "    diffs = [np.linalg.norm(p-Q) for p in P]\n",
    "    mxd = max(diffs)\n",
    "    mnd = min(diffs)\n",
    "    delta = math.log10(mxd-mnd)/mnd\n",
    "    deltas.append( delta )\n",
    "\n",
    "plt.plot(range(2,50),deltas)\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "D_N9K_WesZHu",
    "outputId": "5ebeb34d-998c-4939-95b5-06846152006f"
   },
   "outputs": [],
   "source": [
    "deltas = []\n",
    "for N in range(2,50):\n",
    "    # Generate 1000 random points in N dimensions.\n",
    "    P = [np.random.randint(-100, 100, N) for _ in range(10000)]\n",
    "    Q = np.random.randint(-100,100,N)\n",
    "    diffs = [np.linalg.norm(p-Q) for p in P]\n",
    "    mnd = min(diffs)\n",
    "    delta = math.log10(np.mean(diffs))\n",
    "    deltas.append( delta )\n",
    "\n",
    "plt.plot(range(2,50),deltas)\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Mean Euclidean Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9rrPXJftpWe"
   },
   "source": [
    "Для того, чтобы уменьшить влияние проклятия размерности, используются разные техники:\n",
    " - Feature selection. Выбор наиболее релевантных функций из набора данных может помочь уменьшить размерность и повысить производительность модели.\n",
    " - Снижение размерности. Такие методы, как анализ главных компонент (PCA), линейный дискриминантный анализ (LDA) и t-SNE, могут использоваться для уменьшения размерности данных при сохранении наиболее важной информации.\n",
    " - Регуляризация. Методы регуляризации, такие как L1 и L2, могут помочь предотвратить переобучение в многомерных наборах данных путем добавления штрафного члена к функции потерь.\n",
    " - Ансамблевые методы. Объединение результатов нескольких моделей также может помочь повысить производительность в многомерных наборах данных.\n",
    " - Увеличение размера набора обучающих данных также может помочь преодолеть проклятие размерности, однако сбор большего количества данных может занять много времени и стоит дорого.\n",
    " - Генерация синтетических данных. Генерацию синтетических данных также можно использовать для увеличения размера набора данных, особенно когда сбор большего количества реальных данных невозможен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJErrOewvGZ-"
   },
   "source": [
    "В этом ноутбуке мы рассмотрим некоторые методы оценки важности признаков, а также их отбора на основе важности. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48cOIxYWo9cD"
   },
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCI MAGIC dataset: https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope\n",
    "\n",
    "Описание признаков:\n",
    "\n",
    "- Length: continuous # major axis of ellipse [mm]\n",
    "- Width: continuous # minor axis of ellipse [mm]\n",
    "- Size: continuous # 10-log of sum of content of all pixels [in #phot]\n",
    "- Conc: continuous # ratio of sum of two highest pixels over fSize [ratio]\n",
    "- Conc1: continuous # ratio of highest pixel over fSize [ratio]\n",
    "- Asym: continuous # distance from highest pixel to center, projected onto major axis [mm]\n",
    "- M3Long: continuous # 3rd root of third moment along major axis [mm]\n",
    "- M3Trans: continuous # 3rd root of third moment along minor axis [mm]\n",
    "- Alpha: continuous # angle of major axis with vector to origin [deg]\n",
    "- Dist: continuous # distance from origin to center of ellipse [mm]\n",
    "- Label: g,h # gamma (signal), hadron (background)\n",
    "\n",
    "g = gamma (signal): 12332 \n",
    "\n",
    "h = hadron (background): 6688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuCCBb0DpWuz"
   },
   "outputs": [],
   "source": [
    "columns = np.array([\"Length\", \"Width\", \"Size\", \"Conc\", \"Conc1\", \"Asym\", \"M3Long\", \"M3Trans\", \"Alpha\", \"Dist\"])\n",
    "\n",
    "data = pd.read_csv(pjoin(data_path, \"magic.txt\"),header=None, names=list(columns)+[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[columns].values\n",
    "y = 1 * (data['Label'].values == \"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=11, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Проведите краткий EDA, определите, как выглядят признаки, есть ли пропуски итд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(f_imps, f_names, title=\"\"):\n",
    "    f_imps = np.array(f_imps)\n",
    "    f_names = np.array(f_names)\n",
    "    sort_inds = np.argsort(f_imps)\n",
    "    yy = np.arange(len(f_imps)).astype(int)\n",
    "    plt.barh(yy, f_imps[sort_inds])\n",
    "    plt.yticks(yy, f_names[sort_inds], size=14)\n",
    "    plt.xticks(size=14)\n",
    "    plt.xlabel(\"Feature importance\", size=14)\n",
    "    plt.title(title, size=14)\n",
    "        \n",
    "    # Add importance scores as labels on the horizontal bar chart\n",
    "    for i, v in enumerate(f_imps[sort_inds]):\n",
    "        plt.text(v + 0.01, i, str(round(v, 3)), color=\"black\", fontweight=\"bold\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка важности признаков\n",
    "\n",
    "Оценка важности признаков сильно переплетена с методами интерпретации моделей. Так, существуют модели, интерпретируемые по определению (такие как линейная или дерево решений), для других моделей необходимо использовать внешние (post-hoc) методы.\n",
    "\n",
    "При этом интерпретация разделяется еще и на локальную и глобальную. Первый тип отвечает на вопрос - как принимается решение для конкретного объекта, второй же - анализ модели в целом. В этом ноутбуке мы будем рассматривать именно данный тип интерпретации. \n",
    "\n",
    "Приложение полученных выводов может быть разным, в зависимости от предметной области. например:\n",
    " - какие показатели здоровья важны для отнесения пациента к той или иной группе\n",
    " - какие факторы влияют на вероятность выдачи кредита\n",
    " - как именно строится предпочтение пользователя в выборе фильма\n",
    "итд\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка признаков по определению\n",
    "\n",
    "Классическая линейная модель имеет вид:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\varepsilon$,\n",
    "\n",
    "где каждый коэффициент $\\beta_i$ показывает изменения отклика ;$y$ при увеличении признака $x_i$ на 1. Благодаря этому мы  явно можем количественно оценить важность каждого признака.\n",
    "\n",
    "Логистическая модель описывает отношение шансов:\n",
    "\n",
    "$log {{P(y=1)} \\over {P(y=0)} }= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\varepsilon$\n",
    "\n",
    "где соответственно коэффициенты отражают вклад признака в изменение шансов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для деревьев решений оценка важности чуть сложнее.\n",
    "На значимость каждого признака влияют:\n",
    "- как часто признак использовался в правилах во внутренних узлах дерева\n",
    "- какое число обучающих объектов прошло через узлы, использующие признак\n",
    "- насколько правилам в этих узлах удавалось снизить неопределённость прогнозов\n",
    "\n",
    "Пусть $T(f)$ все ноды, использующие фичу $f$ для разбиения. Tогда, важность фичи $Imp(f)$ of $f$:\n",
    "    $$Imp(f) = \\sum_{t \\in T(f)} n_t \\Delta I(t),$$\n",
    "    $$\\Delta I(t) = I(t) - \\sum_{c \\in children} \\frac{n_c}{n_t} I(c),$$\n",
    "где $n_{t}$ - число объектов в ноде $t$; $I(t)$ – impurity function (gini, cross-entropy, MSE) в ноде $t$\n",
    "\n",
    "Величина $\\Delta I(t)$ представляет собой среднее изменение неопределенности (impurity) при разбиении ноды $t$ по признаку $f$. Она вычисляется как разность между неопределенностью в родительской ноде $I(t)$ и взвешенной суммой неопределенностей в дочерних нодах. Важность признака $Imp(f)$ является взвешенной суммой средних изменений неопределенности по всем нодам, использующим этот признак для разбиения, где веса пропорциональны количеству объектов в каждой ноде. \n",
    "\n",
    "При этом лучше использовать этот метод не по единичному дереву, а по ансамблю, так как деревья склонны переобучаться при построении до самого конца (что нужно для рассчета). В случае ансамбля важности фичей усредняются между деревьями.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пару примеров, на которых можно сравнить разные семейства и их интерпретацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_synth_clf, y_synth_clf = make_classification(\n",
    "    n_samples=1000, n_features=5, n_informative=3, n_redundant=1, \n",
    "    n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "X_synth_clf[:, 0] = X_synth_clf[:, 0] * 1000\n",
    "X_synth_clf[:, 1] = X_synth_clf[:, 1] * 0.1\n",
    "X_synth_clf[:, 2] = X_synth_clf[:, 2] * 100\n",
    "\n",
    "feature_names_clf = [f'Feature_{i}' for i in range(X_synth_clf.shape[1])]\n",
    "\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_synth_clf, y_synth_clf, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_clf_raw.fit(X_train_clf, y_train_clf)\n",
    "lr_importance_clf_raw = np.abs(lr_clf_raw.coef_[0])\n",
    "\n",
    "rf_clf_raw = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf_raw.fit(X_train_clf, y_train_clf)\n",
    "rf_importance_clf_raw = rf_clf_raw.feature_importances_\n",
    "\n",
    "scaler_clf = StandardScaler()\n",
    "X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\n",
    "X_test_clf_scaled = scaler_clf.transform(X_test_clf)\n",
    "\n",
    "lr_clf_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_clf_scaled.fit(X_train_clf_scaled, y_train_clf)\n",
    "lr_importance_clf_scaled = np.abs(lr_clf_scaled.coef_[0])\n",
    "\n",
    "rf_clf_scaled = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf_scaled.fit(X_train_clf_scaled, y_train_clf)\n",
    "rf_importance_clf_scaled = rf_clf_scaled.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(lr_importance_clf_raw, feature_names_clf, \n",
    "                         title=\"Logistic Regression - Classification (Raw Data)\")\n",
    "plot_feature_importances(lr_importance_clf_scaled, feature_names_clf, \n",
    "                         title=\"Logistic Regression - Classification (Scaled Data)\")\n",
    "plot_feature_importances(rf_importance_clf_raw, feature_names_clf, \n",
    "                         title=\"Random Forest - Classification (Raw Data)\")\n",
    "plot_feature_importances(rf_importance_clf_scaled, feature_names_clf, \n",
    "                         title=\"Random Forest - Classification (Scaled Data)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_synth_reg, y_synth_reg = make_regression(\n",
    "    n_samples=1000, n_features=5, n_informative=3, noise=10, random_state=42\n",
    ")\n",
    "feature_names_reg = [f'Feature_{i}' for i in range(X_synth_reg.shape[1])]\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_synth_reg, y_synth_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "lr_reg_raw = LinearRegression()\n",
    "lr_reg_raw.fit(X_train_reg, y_train_reg)\n",
    "lr_importance_reg_raw = np.abs(lr_reg_raw.coef_)\n",
    "\n",
    "rf_reg_raw = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg_raw.fit(X_train_reg, y_train_reg)\n",
    "rf_importance_reg_raw = rf_reg_raw.feature_importances_\n",
    "\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "lr_reg_scaled = LinearRegression()\n",
    "lr_reg_scaled.fit(X_train_reg_scaled, y_train_reg)\n",
    "lr_importance_reg_scaled = np.abs(lr_reg_scaled.coef_)\n",
    "\n",
    "rf_reg_scaled = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg_scaled.fit(X_train_reg_scaled, y_train_reg)\n",
    "rf_importance_reg_scaled = rf_reg_scaled.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(lr_importance_reg_raw, feature_names_reg, \n",
    "                         title=\"Linear Regression - Regression (Raw Data)\")\n",
    "plot_feature_importances(lr_importance_reg_scaled, feature_names_reg, \n",
    "                         title=\"Linear Regression - Regression (Scaled Data)\")\n",
    "plot_feature_importances(rf_importance_reg_raw, feature_names_reg, \n",
    "                         title=\"Random Forest - Regression (Raw Data)\")\n",
    "plot_feature_importances(rf_importance_reg_scaled, feature_names_reg, \n",
    "                         title=\"Random Forest - Regression (Scaled Data)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример на датасете MAGIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_magic_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_magic_raw.fit(X_train, y_train)\n",
    "lr_importance_magic_raw = np.abs(lr_magic_raw.coef_[0])\n",
    "\n",
    "rf_magic_raw = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_magic_raw.fit(X_train, y_train)\n",
    "rf_importance_magic_raw = rf_magic_raw.feature_importances_\n",
    "\n",
    "scaler_magic = StandardScaler()\n",
    "X_train_magic_scaled = scaler_magic.fit_transform(X_train)\n",
    "X_test_magic_scaled = scaler_magic.transform(X_test)\n",
    "\n",
    "lr_magic_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_magic_scaled.fit(X_train_magic_scaled, y_train)\n",
    "lr_importance_magic_scaled = np.abs(lr_magic_scaled.coef_[0])\n",
    "\n",
    "rf_magic_scaled = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_magic_scaled.fit(X_train_magic_scaled, y_train)\n",
    "rf_importance_magic_scaled = rf_magic_scaled.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(lr_importance_magic_raw, columns, \n",
    "                         title=\"Logistic Regression - MAGIC Dataset (Raw Data)\")\n",
    "plot_feature_importances(lr_importance_magic_scaled, columns, \n",
    "                         title=\"Logistic Regression - MAGIC Dataset (Scaled Data)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_feature_importances(rf_importance_magic_raw, columns, \n",
    "                         title=\"Random Forest - MAGIC Dataset (Raw Data)\")\n",
    "plot_feature_importances(rf_importance_magic_scaled, columns, \n",
    "                         title=\"Random Forest - MAGIC Dataset (Scaled Data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: какие из этих признаков можно считать важными? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-agnostic methods\n",
    "\n",
    "###  Information gain \n",
    "Самый простой способ выбора признаков - Information gain (прирост информации). \n",
    "\n",
    "Энтропия целевой переменной Y:\n",
    "$$H(Y) = -\\sum_{i=1}^{c} p(y_i) \\log_2 p(y_i)$$\n",
    "где:\n",
    "$c$ — число классов\n",
    "$p(y_i)$ — вероятность класса $y_i$\n",
    "\n",
    "Условная энтропия Y при известном признаке X:\n",
    "$$H(Y|X) = -\\sum_{j=1}^{m} p(x_j) \\sum_{i=1}^{c} p(y_i|x_j) \\log_2 p(y_i|x_j)$$\n",
    "где:\n",
    "$m$ — число значений признака X\n",
    "$p(x_j)$ — вероятность значения $x_j$\n",
    "$p(y_i|x_j)$ — условная вероятность класса $y_i$ при значении $x_j$\n",
    "\n",
    "Прирост информации - (I(X , Y) = H(X) – H(X | Y)). Это статистика, которая измеряет снижение энтропии (неопределенности) для конкретной функции (таргета) путем деления данных в соответствии с этой характеристикой. \n",
    "\n",
    "\n",
    "Она часто используется в алгоритмах дерева решений, а также имеет другие полезные свойства. Чем выше прирост информации от признака использования признака, тем полезнее он для принятия решений.\n",
    "\n",
    "Для дискретных признаков необходимо следующее:\n",
    "1. Вычислить энтропию целевой переменной: $H(Y)$\n",
    "2. Для каждого признака $X$:\n",
    "   1. Разделить данные по значениям $X$\n",
    "   2. Вычислить условную энтропию $H(Y|X)$\n",
    "   3. Вычислить $IG(Y, X) = H(Y) - H(Y|X)$\n",
    "\n",
    "Вопрос: что дальше необходимо сделать, если это происходит во время построения дерева решений?\n",
    "\n",
    "Для непрерывных признаков сначала необходимо провести дискретизацию(бининг). \n",
    "\n",
    "Вопросы: \n",
    " 1) зависит ли результат от масштаба данных?\n",
    " 2) требует ли такое вычисление каких-то знаний о распределениях в данных? Является ли он параметрическим?\n",
    "\n",
    " В чем проблема этого метода - IG может быть выше для признаков с множеством уникальных значений, соответственно, и оценка важности будет смещена. Кроме того, никак не учитывается их взаимодействие. \n",
    "\n",
    "Для снижения смещения к признакам с большим числом значений используется Information Gain Ratio:\n",
    "$$IGR(Y, X) = \\frac{IG(Y, X)}{H(X)} = \\frac{H(Y) - H(Y|X)}{H(X)}$$\n",
    "Нормализует IG на энтропию признака, уменьшая влияние признаков с высокой энтропией.\n",
    "\n",
    "В sklearn можно оценить важность признаков с помощью mutual_info_regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Information Gain\n",
    "ig = mutual_info_regression(X_train, y_train)\n",
    "\n",
    "# Create a dictionary of feature importance scores\n",
    "feature_scores = {}\n",
    "for i, col in enumerate(list(columns)):\n",
    "    feature_scores[col ] = ig[i]\n",
    "\n",
    "# Sort the features by importance score in descending order\n",
    "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the feature importance scores and the sorted features\n",
    "for feature, score in sorted_features:\n",
    "    print(f\"Feature: {feature}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2xvEMs014Y5"
   },
   "source": [
    "### F-test\n",
    "\n",
    "F-test (F-тест) — это статистический метод для оценки важности признаков, основанный на проверке статистических гипотез. Он сравнивает две модели: нулевую модель (только константа) и альтернативную модель (константа + признак), чтобы определить, вносит ли признак значимый вклад в объяснение целевой переменной.\n",
    "\n",
    "\n",
    "F-test для оценки важности признаков основан на сравнении дисперсий остатков двух моделей [https://online.stat.psu.edu/stat501/lesson/2/2.6]:\n",
    "\n",
    "1. **Нулевая модель (H₀)**: $y = \\beta_0 + \\epsilon$ — модель только с константой (средним значением)\n",
    "2. **Альтернативная модель (H₁)**: $y = \\beta_0 + \\beta_1 x_i + \\epsilon$ — модель с константой и признаком $x_i$\n",
    "\n",
    "Идея заключается в том, что если признак $x_i$ действительно важен, то добавление его в модель должно значительно уменьшить сумму квадратов остатков: \n",
    "\n",
    "SSR - regression sum of squares. Это сумма квадратов отклонений, объясненная моделью. $SSR = \\sum_{i=1}^{n}(\\hat{y}i - \\bar{y})^2$\n",
    "\n",
    "SSE - сумма квадратов ошибок (необъяснённая)ю $SSE = \\sum_{i=1}^{n}(y_i - \\hat{y}i)^2$\n",
    "\n",
    "$MSE = {SSE \\over(n-2)}$ ( т.к. два параметра - коэффициенты при свободном члене и признаке)\n",
    "\n",
    "SSTO = SSR + SSE: $SSTO = \\sum_{i=1}^{n}(y_i - \\bar{y})^2$\n",
    "\n",
    "$ F = {MSR \\over {MSE}} = {{SSR} \\over {SSE \\ (n-2)}} = {{SSR (n - 2)} \\over {SSE}} = {(n - 2){\\sum (\\hat{y}_i - \\overline{y} )^2 } \\over {{\\sum (\\hat{y}_i - y_i )^2 }}} $.\n",
    "\n",
    "\n",
    "F-статистика следует F-распределению с степенями свободы $(1, n-2)$ при справедливости нулевой гипотезы. Т.е.:\n",
    "- **Высокая F-статистика** и **низкий p-value** (< 0.05) указывают на то, что признак статистически значим и вносит важный вклад в модель\n",
    "- **Низкая F-статистика** и **высокий p-value** (> 0.05) указывают на то, что признак не является значимым\n",
    "\n",
    "\n",
    "Признаки ранжируются по убыванию F-статистики или по возрастанию p-value. Кроме того, можно выполнять последовательный тест, сначала найдя важности всех признаков по одному, сравнивая $M0$ и $M1$, далее, выбрав лучшую фичу, использовать $M1$ как базовую модель и создавать уже $M2$ со всем и остальными фичами, и так далее..\n",
    "\n",
    "\n",
    "Если мы знаем $r^2$, то мы можем посчитать F исходя из него:\n",
    "$r^2 = SSR / SSTO => 1 - r^2 = SSE / SSTO $\n",
    "\n",
    "$E[(X_i - mean(X_i)) * (y - mean(y))] / (std(X_i) * std(y))$ \n",
    "\n",
    "Тогда:\n",
    "$$F = \\frac{r^2}{1 - r^2} \\cdot (n - 2)$$\n",
    "\n",
    "В простой линейной регрессии коэффициент детерминации равен квадрату коэффициента корреляции. Это означает, что F-test по сути проверяет значимость линейной корреляции между признаком и целевой переменной.\n",
    "\n",
    "Исходя из вышеперечисленного, какие недостатки у этого метода?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tmnUkqDyMl8",
    "outputId": "ac984ddd-0ef6-475d-d354-079284d019c7"
   },
   "outputs": [],
   "source": [
    "f_statistic, p_values = f_classif(X_train, y_train)\n",
    "feature_scores = {}\n",
    "for i, col in enumerate(list(columns)):\n",
    "    feature_scores[col] = (f_statistic[i], p_values[i])\n",
    "\n",
    "sorted_features = sorted(\n",
    "    feature_scores.items(), key=lambda x: x[1][0], reverse=True\n",
    ")\n",
    "\n",
    "print(\"F-test результаты для датасета MAGIC:\")\n",
    "print(\"=\" * 60)\n",
    "for feature, score in sorted_features:\n",
    "    significance = \"***\" if score[1] < 0.001 else \"**\" if score[1] < 0.01 else \"*\" if score[1] < 0.05 else \"\"\n",
    "    print(f\"Feature: {feature:20s} F-statistic: {score[0]:10.2f} p-value: {score[1]:.2e} {significance}\")\n",
    "\n",
    "ftest_df = pd.DataFrame({\n",
    "    'Feature': [f[0] for f in sorted_features],\n",
    "    'F-statistic': [f[1][0] for f in sorted_features],\n",
    "    'p-value': [f[1][1] for f in sorted_features]\n",
    "})\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "y_pos = np.arange(len(ftest_df))\n",
    "ax1.barh(y_pos, ftest_df['F-statistic'], color='steelblue')\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(ftest_df['Feature'])\n",
    "ax1.set_xlabel('F-statistic', fontsize=12)\n",
    "ax1.set_title('F-test: F-statistic для признаков', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "ax2.barh(y_pos, -np.log10(ftest_df['p-value']), color='coral')\n",
    "ax2.axvline(x=-np.log10(0.05), color='red', linestyle='--', linewidth=2, label='p-value = 0.05')\n",
    "ax2.axvline(x=-np.log10(0.01), color='orange', linestyle='--', linewidth=2, label='p-value = 0.01')\n",
    "ax2.axvline(x=-np.log10(0.001), color='darkred', linestyle='--', linewidth=2, label='p-value = 0.001')\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(ftest_df['Feature'])\n",
    "ax2.set_xlabel('-log10(p-value)', fontsize=12)\n",
    "ax2.set_title('F-test: Уровень значимости (-log10 p-value)', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.legend()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nКоличество значимых признаков (p-value < 0.05): {sum(ftest_df['p-value'] < 0.05)}\")\n",
    "print(f\"Количество высокозначимых признаков (p-value < 0.01): {sum(ftest_df['p-value'] < 0.01)}\")\n",
    "print(f\"Количество очень высокозначимых признаков (p-value < 0.001): {sum(ftest_df['p-value'] < 0.001)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-test также может использоваться для задач регрессии с помощью функции `f_regression`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос: Какие из столбцов можно считать важными, основываясь на этом тесте? Поясните свой ответ\n",
    "\n",
    "Задание: \n",
    "1) Постройте модель логистической регрессии, используя все признаки из набора.\n",
    "2) Также постройте модели для выбранных наборов признаков. \n",
    "3) Сравните модели по точности, используя кросс-валидацию. \n",
    "\n",
    "Задание: Покажите, как будет меняться оценка важности признаков линейной модели при добавлении выбросов и нелинейных зависимостей. Сравните с оценкой по коэффициентам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HsQPgQHVBEZQ"
   },
   "outputs": [],
   "source": [
    "def print_feature_names(features, original_feature_names: list  | None = None):\n",
    "  feature_names = [str(original_feature_names[int(f.strip(\"pixel\"))]) for f in features]\n",
    "  print(f\"Selected features: {feature_names}\")\n",
    "\n",
    "\n",
    "def select_features(X_train, y_train, X_test, function, k: int = 5, features: list  | None = None):\n",
    " # configure to select a subset of features\n",
    " fs = SelectKBest(score_func=function, k=k)\n",
    " fs.fit(X_train, y_train)\n",
    " print_feature_names(fs.get_feature_names_out(), features)\n",
    " X_train_fs = fs.transform(X_train)\n",
    " X_test_fs = fs.transform(X_test)\n",
    " return X_train_fs, X_test_fs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5_ynFIjAVOT",
    "outputId": "fef38992-1ead-49af-e8da-0969f93e717b"
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "cross_val_score(\n",
    "    model, X, y, scoring=\"accuracy\", cv=5\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "qz5hwxNrBafl",
    "outputId": "29407dff-b769-43fb-bd08-1d16c550644d"
   },
   "outputs": [],
   "source": [
    "results = {\"num_features\": [], \"acc\": [], \"roc_auc\": []}\n",
    "for i in range(1, len(columns)):\n",
    "    results[\"num_features\"].append(i)\n",
    "    # feature selection\n",
    "    X_train_fs, X_test_fs, fs = select_features(\n",
    "        X_train, y_train, X_test, f_classif, k=i, features = list(columns)\n",
    "        )\n",
    "    # fit the model\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train_fs, y_train)\n",
    "    # evaluate the model\n",
    "    yhat = model.predict(X_test_fs)\n",
    "    probs = model.predict_proba(X_test_fs)[:, 1]\n",
    "    # evaluate predictions\n",
    "    results[\"acc\"].append(accuracy_score(y_test, yhat))\n",
    "    results[\"roc_auc\"].append(roc_auc_score(y_test, probs))\n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Реализуйте такой метод, используя линейную модель с регуляризацией.\n",
    "\n",
    "**Бонусное задание**: Сделайте метрику аргументом, проведите тесты с линейной моделью, деревом решений, случайным лесом, двумя метриками. Постройте таблицу со сравнением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_imp_general(X_test, y_test, model):\n",
    "    feature_importances = []\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    q_0 = accuracy_score(y_test, y_test_proba)\n",
    "    for i in range(X_test.shape[1]):\n",
    "\n",
    "        # do not forget to make a copy of X_test!\n",
    "        X_test_copy = X_test.copy()\n",
    "\n",
    "        # shuffle values of the i-th feature\n",
    "        ### YOUR CODE\n",
    "        #\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        X_test_copy = np.nan_to_num(X_test_copy)\n",
    "        y_hat = model.predict(X_test_copy)\n",
    "        q_r = accuracy_score(y_test, y_hat)\n",
    "\n",
    "        importance = None # !! your code (calculate importance according to algorithm)\n",
    "        feature_importances.append(importance)\n",
    "\n",
    "    return np.array(f_imps_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Importance\n",
    "\n",
    "**Permutation Importance** измеряет важность признака как ухудшение производительности модели при случайном перемешивании значений этого признака. Если признак важен, его перемешивание должно значительно снизить качество предсказаний модели.\n",
    "\n",
    "Математически, для признака $f$ важность определяется как:\n",
    "\n",
    "$$Imp(f) = Q_0 - Q_f$$\n",
    "\n",
    "где:\n",
    "- $Q_0$ — базовая метрика качества модели на исходных данных\n",
    "- $Q_f$ — метрика качества модели после перемешивания значений признака $f$\n",
    "\n",
    "\n",
    "Как видно, для разных моделей оценки сильно отличаются. \n",
    "Общий метод оценки важности: \n",
    "1) Обучите свою модель\n",
    "2) Рассчитайте метрику качества  $Q_o$  на тестовом наборе\n",
    "3) Для признака $f$:\n",
    "     1) Замените значения случайными значениями из того же распределения (выполните случайное перемешивание)\n",
    "     2) Рассчитайте метрику качества $Q_r$ на тестовом наборе \n",
    "     3) Оцените важность признака: $Imp(f) = Q_0 - Q_r$\n",
    "\n",
    "\n",
    "Для получения более стабильной оценки процесс повторяется $n$ раз (обычно $n=5$ или $n=10$), и берется среднее значение:\n",
    "\n",
    "$$Imp(f) = \\frac{1}{n} \\sum_{i=1}^{n} (Q_0 - Q_{f,i})$$\n",
    "\n",
    "где $Q_{f,i}$ — метрика качества после $i$-го перемешивания признака $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_permutation_importance(model, X_test, y_test, feature_names, \n",
    "                                n_repeats=10, random_state=42, scoring='accuracy', \n",
    "                                n_jobs=-1, title='', xlabel='Permutation Importance', \n",
    "                                color='steelblue', figsize=(10, 6), show_plot=True):\n",
    "    perm_importance = permutation_importance(\n",
    "        model, X_test, y_test,\n",
    "        n_repeats=n_repeats, random_state=random_state, scoring=scoring, n_jobs=n_jobs\n",
    "    )\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': perm_importance.importances_mean,\n",
    "        'Std': perm_importance.importances_std\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    if show_plot:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        y_pos = np.arange(len(importance_df))\n",
    "        ax.barh(y_pos, importance_df['Importance'], xerr=importance_df['Std'], \n",
    "                capsize=5, color=color)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(importance_df['Feature'])\n",
    "        ax.set_xlabel(xlabel, fontsize=12)\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация на синтетических данных:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_synth_clf_perm, y_synth_clf_perm = make_classification(\n",
    "    n_samples=1000, n_features=6, n_informative=3, n_redundant=1,\n",
    "    n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "X_train_clf_perm, X_test_clf_perm, y_train_clf_perm, y_test_clf_perm = train_test_split(\n",
    "    X_synth_clf_perm, y_synth_clf_perm, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "rf_clf_perm = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf_perm.fit(X_train_clf_perm, y_train_clf_perm)\n",
    "\n",
    "feature_names_clf_perm = [f'Feature_{i}' for i in range(X_synth_clf_perm.shape[1])]\n",
    "importance_df_clf = plot_permutation_importance(\n",
    "    rf_clf_perm, X_test_clf_perm, y_test_clf_perm, feature_names_clf_perm,\n",
    "    n_repeats=10, random_state=42, scoring='accuracy', n_jobs=-1,\n",
    "    title='Permutation Importance - Классификация (Random Forest)'\n",
    ")\n",
    "\n",
    "print(\"Permutation Importance для классификации:\")\n",
    "print(importance_df_clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим Permutation Importance к датасету MAGIC и сравним результаты с другими методами оценки важности признаков.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_magic_perm = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_magic_perm.fit(X_train, y_train)\n",
    "\n",
    "importance_df_magic = plot_permutation_importance(\n",
    "    rf_magic_perm, X_test, y_test, columns,\n",
    "    n_repeats=10, random_state=42, scoring='roc_auc', n_jobs=-1,\n",
    "    title='Permutation Importance - MAGIC Dataset (Random Forest)',\n",
    "    xlabel='Permutation Importance (ROC-AUC decrease)',\n",
    "    color='steelblue'\n",
    ")\n",
    "\n",
    "print(\"Permutation Importance для датасета MAGIC:\")\n",
    "print(importance_df_magic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним Permutation Importance с встроенной важностью Random Forest для датасета MAGIC:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builtin_importance_magic = rf_magic_perm.feature_importances_\n",
    "\n",
    "comparison_magic_df = pd.DataFrame({\n",
    "    'Feature': columns,\n",
    "    'Permutation Importance': importance_df_magic[\"Importance\"],\n",
    "    'Built-in Importance': builtin_importance_magic\n",
    "}).sort_values('Permutation Importance', ascending=False)\n",
    "\n",
    "print(\"Сравнение методов для датасета MAGIC:\")\n",
    "print(comparison_magic_df)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "sort_idx = comparison_magic_df.index\n",
    "y_pos = np.arange(len(comparison_magic_df))\n",
    "\n",
    "ax1.barh(y_pos, comparison_magic_df['Permutation Importance'], \n",
    "         xerr=importance_df_magic.loc[sort_idx, \"Std\"], capsize=5, color='steelblue')\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(comparison_magic_df['Feature'])\n",
    "ax1.set_xlabel('Permutation Importance', fontsize=12)\n",
    "ax1.set_title('Permutation Importance', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.barh(y_pos, comparison_magic_df['Built-in Importance'], color='coral')\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(comparison_magic_df['Feature'])\n",
    "ax2.set_xlabel('Built-in Importance', fontsize=12)\n",
    "ax2.set_title('Встроенная важность (Random Forest)', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также применим Permutation Importance к линейной модели для сравнения:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_magic_perm = StandardScaler()\n",
    "X_train_magic_scaled_perm = scaler_magic_perm.fit_transform(X_train)\n",
    "X_test_magic_scaled_perm = scaler_magic_perm.transform(X_test)\n",
    "\n",
    "lr_magic_perm = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_magic_perm.fit(X_train_magic_scaled_perm, y_train)\n",
    "\n",
    "importance_df_magic_lr = plot_permutation_importance(\n",
    "    lr_magic_perm, X_test_magic_scaled_perm, y_test, columns,\n",
    "    n_repeats=10, random_state=42, scoring='roc_auc', n_jobs=-1,\n",
    "    title='Permutation Importance - MAGIC Dataset (Logistic Regression)',\n",
    "    xlabel='Permutation Importance (ROC-AUC decrease)',\n",
    "    color='green'\n",
    ")\n",
    "\n",
    "print(\"Permutation Importance для Logistic Regression (MAGIC):\")\n",
    "print(importance_df_magic_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из важных ограничений Permutation Importance является его поведение при наличии коррелированных признаков. Если два признака сильно коррелированы, то при перемешивании одного из них модель может использовать информацию из коррелированного признака, что приведет к занижению важности обоих признаков.При наличии сильно коррелированных признаков важно интерпретировать результаты с осторожностью. Можно рассмотреть группировку коррелированных признаков или использование других методов интерпретации.\n",
    "\n",
    "Кроме того, Permutation Importance зависит от выбранной метрики качества. Выбор метрики должен соответствовать бизнес-задаче. Например, если важна точность предсказания определенного класса, следует использовать соответствующую метрику.\n",
    "\n",
    "Иногда Permutation Importance может быть отрицательной. Это означает, что перемешивание признака **улучшило** качество модели. Возможные причины:\n",
    "\n",
    "1. **Переобучение**: Модель переобучилась на шум в этом признаке\n",
    "2. **Случайность**: При малом количестве повторений возможны случайные флуктуации\n",
    "3. **Корреляция**: При наличии коррелированных признаков перемешивание одного может случайно улучшить предсказания\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мультиколлинеарность\n",
    "\n",
    "При наличии сильно коррелированных признаков permutation importance может недооценивать важность признаков. Если два признака сильно коррелированы, при перемешивании одного из них модель может использовать информацию из коррелированного признака, что приводит к занижению важности обоих.\n",
    "\n",
    "Для корректной оценки важности признаков при мультиколлинеарности можно использовать кластеризацию признаков по корреляции и оценивать важность кластеров признаков, а не отдельных признаков.\n",
    "\n",
    "Рассмотрим эту ситуацию на примере wbcd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data_bc =  pd.read_csv(pjoin(data_path, \"wdbc\", \"data.csv\"))\n",
    "X_bc = data_bc.drop(\"diagnosis\", axis=1)\n",
    "y_bc = data_bc[\"diagnosis\"]\n",
    "\n",
    "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(\n",
    "    X_bc, y_bc, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "corr_matrix = X_train_bc.corr().abs()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', center=0, square=True, \n",
    "            linewidths=0.5, cbar_kws={\"shrink\": 0.8}, fmt='.2f')\n",
    "plt.title('Матрица корреляций признаков (Breast Cancer)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_bc.fit(X_train_bc, y_train_bc)\n",
    "print(\n",
    "    \"Accuracy on test data with all feature: {:.2f}\".format(\n",
    "        rf_bc.score(X_test_bc, y_test_bc)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df_bc = plot_permutation_importance(\n",
    "    rf_bc, X_test_bc, y_test_bc, X_bc.columns,\n",
    "    n_repeats=10, random_state=42, scoring='roc_auc',\n",
    "    show_plot=False\n",
    ")\n",
    "\n",
    "rf_importance = rf_bc.feature_importances_\n",
    "importance_df_bc['RF Importance'] = rf_importance\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "y_pos = np.arange(len(importance_df_bc))\n",
    "ax1.barh(y_pos, importance_df_bc['Importance'], \n",
    "        xerr=importance_df_bc['Std'], capsize=3, color='steelblue')\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(importance_df_bc['Feature'], fontsize=9)\n",
    "ax1.set_xlabel('Permutation Importance (ROC-AUC decrease)', fontsize=12)\n",
    "ax1.set_title('Permutation Importance - Breast Cancer Dataset', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2.barh(y_pos, importance_df_bc['RF Importance'], color='coral')\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(importance_df_bc['Feature'], fontsize=9)\n",
    "ax2.set_xlabel('RF Importance', fontsize=12)\n",
    "ax2.set_title('IG Feature Importance', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из способов обработки мультиколлинеарных признаков — выполнение иерархической кластеризации на основе корреляций, выбор порога и сохранение одного признака из каждого кластера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "corr = np.nan_to_num(spearmanr(X_bc).correlation) # don't need target\n",
    "# Ensure the correlation matrix is symmetric - it's important to use it as distance\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "distance_matrix = 1 - np.abs(corr)\n",
    "\n",
    "# We perform Ward clustering, because it builds dense and similar-size clusters.\n",
    "dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "dendro = hierarchy.dendrogram(\n",
    "    dist_linkage, labels=X_bc.columns.tolist(), ax=ax1, leaf_rotation=90\n",
    ")\n",
    "dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "ax2.set_xticks(dendro_idx)\n",
    "ax2.set_yticks(dendro_idx)\n",
    "ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание: постройте карту корреляций с дендрограммой силами sklearn. Насколько отличаются дендрограммы? По возможности объясните разницу (или укажите, что там используется тот же алгоритм)\n",
    "\n",
    "Далее мы можем выбрать необходимый нам уровень кластеров, чтобы выбрать признак для оценки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "cluster_id_to_feature_ids = defaultdict(list)\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "selected_features = [X_train_bc.columns[v[1]] for v in cluster_id_to_feature_ids.values()]\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "X_train_sel = X_train_bc[selected_features]\n",
    "X_test_sel = X_test_bc[selected_features]\n",
    "\n",
    "rf_sel = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_sel.fit(X_train_sel, y_train_bc)\n",
    "print(\n",
    "    \"Accuracy on test data with features removed: {:.2f}\".format(\n",
    "        rf_sel.score(X_test_sel, y_test_bc)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importance_df_bc = plot_permutation_importance(\n",
    "    rf_sel, X_test_sel, y_test_bc, selected_features,\n",
    "    n_repeats=10, random_state=42, scoring='roc_auc',\n",
    "    show_plot=False\n",
    ")\n",
    "\n",
    "rf_importance = rf_sel.feature_importances_\n",
    "importance_df_bc['RF Importance'] = rf_importance\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "y_pos = np.arange(len(importance_df_bc))\n",
    "ax1.barh(y_pos, importance_df_bc['Importance'], \n",
    "        xerr=importance_df_bc['Std'], capsize=3, color='steelblue')\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(importance_df_bc['Feature'], fontsize=9)\n",
    "ax1.set_xlabel('Permutation Importance (ROC-AUC decrease)', fontsize=12)\n",
    "ax1.set_title('Permutation Importance - Breast Cancer Dataset', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax2.barh(y_pos, importance_df_bc['RF Importance'], color='coral')\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(importance_df_bc['Feature'], fontsize=9)\n",
    "ax2.set_xlabel('RF Importance', fontsize=12)\n",
    "ax2.set_title('IG Feature Importance', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание: Покажите влияние мультиколлинеарности на F-statistic, проверив признаки WDBC в разном порядке. Как меняется важность при смене порядка?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJjvJyVi9NaF"
   },
   "source": [
    "## Feature selection\n",
    "Как же перейти от простой оценки важности признаков к поиску такого подмножества, которое приведет к наибольшей точности модели?\n",
    "Можно использовать рекурсиввный выбор признаков (Recursive feature selection, или RFE).\n",
    "RFE работает путем поиска подмножества признаков, начиная со всех признаков в наборе и постепенно удаляя самые незначимые, пока не останется желаемое количество.\n",
    "\n",
    "Это достигается путем подбора модели для оценки, ранжирования признаков по важности, отбрасывания наименее важного признака и повторного обучения модели. Этот процесс повторяется до тех пор, пока не останется заданное количество признаков. Характеристики оцениваются либо с использованием предоставленной модели, либо с использованием статистического метода, такого как f-score или IG.\n",
    "\n",
    "В sklearn реализован класс трансформера для такого подбора, при этом подюор может быть отделен от основной модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5cJqvjZ6nYu"
   },
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=5)\n",
    "model = LogisticRegression()\n",
    "#pipeline = Pipeline(steps=[('sc',scaler),('s',rfe),('m',model)])\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "pipeline.fit(X_train, y_train)\n",
    "print_feature_names(rfe.get_feature_names_out(), columns)\n",
    "# evaluate the model\n",
    "y_hat = pipeline.predict(X_test)\n",
    "# evaluate predictions\n",
    "acc = accuracy_score(y_test, y_hat)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Реализуйте данный метод вручную. Бонусное задание: реализуйте класс трансформера для подбора и/или конфигурируемые метрику и модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_features_set(X_train, y_train, X_test, y_test, model, columns):\n",
    "    \"\"\"\n",
    "    Recursive feature elimination\n",
    "    \"\"\"\n",
    "    X_train_curr = X_train.copy()\n",
    "    X_test_curr  = X_test.copy()\n",
    "    f_names_curr = columns.copy()\n",
    "\n",
    "    # define a list for the feature importances\n",
    "    scores = []\n",
    "    # for each feature in the sample estimate its importance\n",
    "    for i in range(X_test.shape[1]):\n",
    "        # fit the model using current set of festures\n",
    "        # get feature importances\n",
    "        ### YOUR CODE\n",
    "        #\n",
    "        # importances = ...\n",
    "        ### YOUR CODE\n",
    "        yhat = model.predict(X_test_curr)\n",
    "        score = accuracy_score(y_test, y_hat)\n",
    "        print(\"Score: \", np.round(score, 4))\n",
    "        scores.append(score)\n",
    "        # remove feature with the least importance\n",
    "        min_importance = importances.min()\n",
    "        X_train_curr = X_train_curr[:, importances > min_importance]\n",
    "        X_test_curr  = X_test_curr[:, importances > min_importance]\n",
    "        f_names_curr = f_names_curr[importances > min_importance]\n",
    "    return scores, f_names_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, f_names  = find_best_features_set(X_train, y_train, X_test, y_test, model, columns)\n",
    "nf = np.arange(1, len(scores)+1)[::-1]\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.plot(nf, scores, linewidth=3)\n",
    "plt.scatter(nf, scores, linewidth=3)\n",
    "\n",
    "plt.xlabel(\"Number of features\", size=16)\n",
    "plt.ylabel(\"Score\", size=16)\n",
    "plt.xticks(size=16)\n",
    "plt.yticks(size=16)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EsjypD0omIj"
   },
   "source": [
    "### PCA\n",
    "\n",
    "[Ноутбук Евгения Соколова](https://github.com/esokolov/ml-course-hse/blob/master/2022-spring/seminars/sem14_pca_tsne.ipynb)\n",
    "\n",
    "Выделение новых признаков путем их отбора часто дает плохие результаты, и в некоторых ситуациях такой подход практически бесполезен. Например, если мы работаем с изображениями, у которых признаками являются яркости пикселей,\n",
    "невозможно выбрать небольшой поднабор пикселей, который дает хорошую информацию о содержимом картинки. Поэтому признаки нужно как-то комбинировать.\\n\",\n",
    "\n",
    "__Метод главных компонент__ &mdash; один из самых интуитивно простых и часто используемых методов для снижения размерности данных и проекции их на ортогональное подпространство признаков. В рамках метода делается два важных упрощения задачи:\n",
    "1. игнорируется целевая переменная;\n",
    "2. строится линейная комбинация признаков.\n",
    "П. 1 на первый взгляд кажется довольно странным, но на практике обычно не является таким уж плохим. Это связано с тем, что часто данные устроены так, что имеют какую-то внутреннюю структуру в пространстве меньшей размерности, которая никак не связана с целевой переменной. Поэтому и оптимальные признаки можно строить не глядя на ответ.\n",
    "П. 2 тоже сильно упрощает задачу, но далее мы научимся избавляться от него.\n",
    "\n",
    "PCA ищет линейную комбинацию переменных, чтобы мы могли извлечь максимальную дисперсию из них. По завершении этого процесса он удаляет ее и ищет другую линейную комбинацию (фактор), которая дает объяснение максимальной пропорции оставшейся дисперсии, что приводит к ортогональным факторам. Это повторяется столько раз, чтобы достигнуть целевого числа объясненной дисперсии или числа факторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теория\n",
    "Кратко вспомним, что делает этот метод.\n",
    "Пусть $X$ &mdash; матрица объекты-признаки, с нулевым средним каждого признака, а $w$ &mdash; некоторый единичный вектор. Тогда $Xw$ задает величину проекций всех объектов на этот вектор. Далее ищется вектор, который дает наибольшую дисперсию полученных проекций (то есть наибольшую дисперсию вдоль этого направления):\n",
    "$$\\max_{w: \\|w\\|=1} \\| Xw \\|^2 =  \\max_{w: \\|w\\|=1} w^T X^T X w$$\n",
    "Подходящий вектор тогда равен собственному вектору матрицы $X^T X$ с наибольшим собственным значением. После этого все пространство проецируется на ортогональное дополнение к вектору $w$ и процесс повторяется.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(314512)\n",
    "\n",
    "data_synth_1 = np.random.multivariate_normal(\n",
    "    mean=[0, 0],\n",
    "    cov=[[4, 0],\n",
    "         [0, 1]],\n",
    "    size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь изобразим точки выборки на плоскости и применим к ним PCA для нахождения главных компонент. В результате работы PCA из sklearn в dec.components_ будут лежать главные направления (нормированные), а в dec.explained_variance_ — дисперсия, которую объясняет каждая компонента. Изобразим на нашем графике эти направления, умножив их на дисперсию для наглядного отображения их значимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_show(dataset):\n",
    "    plt.scatter(*zip(*dataset), alpha=0.5)\n",
    "\n",
    "    dec = PCA()\n",
    "    dec.fit(dataset)\n",
    "    ax = plt.gca()\n",
    "    for comp_ind in range(dec.components_.shape[0]):\n",
    "        component = dec.components_[comp_ind, :]\n",
    "        var = dec.explained_variance_[comp_ind]\n",
    "        start, end = dec.mean_, component * var\n",
    "        ax.arrow(start[0], start[1], end[0], end[1],\n",
    "                 head_width=0.2, head_length=0.4, fc='r', ec='r')\n",
    "\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "PCA_show(data_synth_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df_multi, groups_multi = plot_permutation_importance_multicollinear(\n",
    "    rf_magic_perm, X_test, y_test, columns,\n",
    "    correlation_threshold=0.7,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что PCA все правильно нашел. Но это, конечно, можно было сделать и просто посчитав дисперсию каждого признака. Повернем наши данные на некоторый фиксированный угол и проверим, что для PCA это ничего не изменит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.pi / 6\n",
    "rotate = np.array([\n",
    "        [np.cos(angle), - np.sin(angle)],\n",
    "        [np.sin(angle), np.cos(angle)],\n",
    "    ])\n",
    "data_synth_2 = rotate.dot(data_synth_1.T).T\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "PCA_show(data_synth_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже пара примеров, где PCA отработал не так хорошо (в том смысле, что направления задают не очень хорошие признаки).\n",
    "\n",
    "Вопрос: почему так произошло?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_moons, make_blobs\n",
    "\n",
    "np.random.seed(54242)\n",
    "\n",
    "data_synth_bad = [\n",
    "    make_circles(n_samples=1000, factor=0.2, noise=0.1)[0]*2,\n",
    "    make_moons(n_samples=1000, noise=0.1)[0]*2,\n",
    "    make_blobs(n_samples=1000, n_features=2, centers=4)[0]/5,\n",
    "    np.random.multivariate_normal(\n",
    "        mean=[0, 1.5],\n",
    "        cov=[[3, 1],\n",
    "             [1, 1]],\n",
    "        size=1000),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "rows, cols = 2, 2\n",
    "for i, data in enumerate(data_synth_bad):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    PCA_show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrgGBxPP9WFO"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkxCayiepaHk"
   },
   "source": [
    "Доп.задание : Запустите предыдущий метод с учетом скейлинга. Поменяется ли результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7AaESSvpZXI"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 5)\n",
    "pipeline = Pipeline(steps=[('sc', scaler), ('pca',pca),('m',model)])\n",
    "# evaluate model\n",
    "pipeline.fit(X_train, y_train)\n",
    "pcas = \" \".join(f\"{i:.2}\" for i in pca.explained_variance_ratio_)\n",
    "print(f\"Explained {pcas} ({pca.explained_variance_ratio_.sum():.2f}) ratio of variance\")\n",
    "# evaluate the model\n",
    "y_hat = pipeline.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, y_hat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt8A-MfMs1hu"
   },
   "source": [
    "Почему важно использование скейлинга перед проведением PCA?\n",
    "Если один объект варьируется больше, чем другие, только из-за их соответствующих масштабов, PCA определит, что такой объект доминирует в направлении главных компонентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlSjGOwPp-d5"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 5)\n",
    "pipeline = Pipeline(steps=[('pca',pca),('m',model)])\n",
    "# evaluate model\n",
    "pipeline.fit(X_train, y_train)\n",
    "pcas = \" \".join(f\"{i:.2}\" for i in pca.explained_variance_ratio_)\n",
    "print(f\"Explained {pcas} ({pca.explained_variance_ratio_.sum():.2f}) ratio of variance\")\n",
    "# evaluate the model\n",
    "y_hat = pipeline.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, y_hat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc1Fz99tuYV8"
   },
   "source": [
    " Какой метод наиболее страдает от проклятия размерности?\n",
    " KNN.\n",
    "\n",
    " Покажем это."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5-m_hbZ4cNF"
   },
   "source": [
    "Рассмотрим набор MNIST - набор рукописных чисел от 0 до 9. Это один из классических наборов для компьютернорго зроения, состоящий из 100 тысяч черно-белых изображений 28х28. Однако матрицы можно и вытянуть в вектор, чтобы предсказывать класс в табличном формате. Тогда мы получим пространство признаков размера 784!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyZuVMWEuFFY"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "\n",
    "X = mnist[\"data\"].astype('float64')\n",
    "X.reset_index()\n",
    "y = mnist[\"target\"].astype('int64')\n",
    "y.reset_index()\n",
    "\n",
    "print(\"\\nNo. of Samples: \", X.shape)\n",
    "print(\"No. of Labels: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3TMMZpB4-Wi"
   },
   "source": [
    "Так выглядит набор данных в табличном виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYzal5vP3Pvz"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kK0ItAvi5Cmg"
   },
   "source": [
    "Рассмотрим случайный семпл в данных (преобразовав обратно в матрицу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKJTMUxw2mDz"
   },
   "outputs": [],
   "source": [
    "random_digit = X.loc[10,:].values\n",
    "\n",
    "random_digit_image = random_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(random_digit_image, cmap = 'gray', interpolation=\"nearest\")\n",
    "#plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5uLQXRM5Joj"
   },
   "source": [
    "Отнормируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYHAcyeR2rMV"
   },
   "outputs": [],
   "source": [
    "X /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbAPTh3I2ruG"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCjv5I1i5NeM"
   },
   "source": [
    "Заодно с обучением модели вспомним о поиске гиперпараметров. Что это такое?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmmXqGws2r6M"
   },
   "source": [
    "param_grid = {'n_neighbors': [3, 4, 5, 9], 'p': [1, 2, 1000], 'weights': [\"uniform\", \"distance\"]}\n",
    "\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "knn_cv = GridSearchCV(knn_clf, param_grid, scoring='f1_micro', cv=5, verbose=1, n_jobs=-1)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "params_optimal_knn = knn_cv.best_params_\n",
    "\n",
    "print(\"Best Score: %f\" % knn_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_knn)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oLX7reE8Nar"
   },
   "source": [
    "Так как данных очень много, сделаем равномерно засемплим 1000 точек. В общем случае это плохая практика. Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZgmTcG58Nzs"
   },
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(X.index, 10000, replace=False)\n",
    "X_rus = X.loc[random_indices]\n",
    "y_rus = y.loc[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxzrVSGw2sMi"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NPp7UKt2stX"
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors': [3, 4], 'p': [2], 'weights': [\"distance\"]}\n",
    "\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "knn_cv = GridSearchCV(knn_clf, param_grid, scoring='f1_micro', cv=5, verbose=1, n_jobs=-1)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "params_optimal_knn = knn_cv.best_params_\n",
    "\n",
    "print(\"Best Score: %f\" % knn_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_knn)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TrSQfcM75g4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "knn = KNeighborsClassifier(**params_optimal_knn)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_train_predicted = knn.predict(X_train)\n",
    "\n",
    "train_accuracy_knn = np.mean(y_train_predicted == y_train)\n",
    "print(\"\\nTraining Accuracy: \", train_accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_zj69cH9dr-"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "y_train_pred = cross_val_predict(knn, X_train, y_train, cv=5)\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "\n",
    "test_accuracy_knn = np.mean(y_test_predicted == y_test)\n",
    "print(\"\\nTraining Accuracy: \", test_accuracy_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUMrtWsi9nVb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# The accuracy of the model\n",
    "test_accuracy_knn = knn.score(X_test, y_test)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy_knn)\n",
    "\n",
    "\n",
    "# No. of Correct Predictions\n",
    "y_test_predicted = knn.predict(X_test)\n",
    "print(\"\\nNo. of correct predictions (Test): %d/%d\" % (np.sum(y_test_predicted == y_test), len(y_test)))\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (Test Data):\\n\", confusion_matrix(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d4fOpdZ92UL"
   },
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, y_test_predicted, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N6phuYv-TPh"
   },
   "source": [
    "Попробуем уменьшить влияние проклятия размерности с помощью уменьшения размерности, использовав метод PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDHPls-d93jl"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "\n",
    "print(\"Number of Principle Components: \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBKiZN_W-CrS"
   },
   "outputs": [],
   "source": [
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIIDgyc5-Eo2"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=4, p=2, weights=\"distance\")\n",
    "\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "\n",
    "y_test_predicted_pca_knn = knn_pca.predict(X_test_pca)\n",
    "print(\"KNN (PCA): Test Accuracy: \", accuracy_score(y_test, y_test_predicted_pca_knn))\n",
    "\n",
    "print(\"\\nKNN (PCA): Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_pca_knn))\n",
    "\n",
    "print(\"\\nKNN (PCA): Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_pca_knn))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK5rCzrt-JzJ"
   },
   "source": [
    "Мы видим, что после уменьшения размерности мы можем снизить стоимость расчета расстояния в K-NN, что приводит к значительному сокращению времени обучения. При этом мы не потеряли в точности, а даже выиграли!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXt5fDBz_SYx"
   },
   "source": [
    "Для сравнения времени обучения рассмотрим метод, построенный не на расстоянии - случайный лес. Эта модель будет страдать от проклятия размерности в терминах времени меньше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZA2XP_o-M5Q"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", \n",
    "                                    max_depth=32, class_weight=\"balanced\", oob_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_predicted_rf = forest_clf.predict(X_test)\n",
    "print(\"Random Forest: Test Accuracy: \", accuracy_score(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bH71bPpS_j26"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", \n",
    "                                    max_depth=32, class_weight=\"balanced\", oob_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "forest_clf.fit(X_train_pca, y_train)\n",
    "\n",
    "y_test_predicted_rf = forest_clf.predict(X_test_pca)\n",
    "print(\"Random Forest: Test Accuracy: \", accuracy_score(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Classification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImVtYu03Av6d"
   },
   "outputs": [],
   "source": [
    "X_train_big, X_test_big, y_train_big, y_test_big = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train_big.shape, y_test_big.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jetQ6JAO_rfY"
   },
   "source": [
    "%%time\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", \n",
    "                                    max_depth=32, class_weight=\"balanced\", oob_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "forest_clf.fit(X_train_big, y_train_big)\n",
    "\n",
    "y_test_predicted_rf = forest_clf.predict(X_test_big)\n",
    "print(\"Random Forest: Test Accuracy: \", accuracy_score(y_test_big, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Test Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_big, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\nRandom Forest: Classification Report:\")\n",
    "print(classification_report(y_test_big, y_test_predicted_rf))\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuDLiENxCz70"
   },
   "source": [
    "Как видно, обучение произошло гораздо быстрее, при этом точность алгоритма даже выше, чем у KNN"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VqtFGGjec-ZS"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_2025",
   "language": "python",
   "name": "ml_2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
